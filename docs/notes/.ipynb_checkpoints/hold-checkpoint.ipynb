{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More and more scientists are applying deep neural networks to their data problems every day. Usually, the factors that determine the characteristics of that network are largely based on their own expertise, and their ability to alter available deep learning software to fit the needs of their data. In general, however, it may not be clear without significant experience the criteria by which one network architecture is appropriate.\n",
    "\n",
    "In order to address this problem, the field of neuroevolution has strove to design representations of neural network architectures, in an attempt to create systems that automatically determine the model that would provide good performance by some measure. The field itself takes inspiration from biology - biological neural networks are a product of not only within-generation learning but also development and evolution. Natural selection determines classes of solutions that persist in a particular niche, and development and learning does what it can with those initial conditions.\n",
    "\n",
    "When applying this perspective to the design of artificial neural networks, that is, that a particular deep learning model exists as one solution in a space of all possible solutions, its location within that space is entirely dependent on what we choose as an effective representation for that solution.\n",
    "\n",
    "Early in the field, it was common to represent an artificial neural network with a string of bits, where each gene in this bitstring genome represented the individual weights for each connection in the final network, an approach referred to as direct encoding. This approach quickly was seen as problematic as larger models with many more parameters were needed to solve more complex problems. As the sizes of these solutions grow, their genomes would also have to grow at the same rate. In some of the largest convolutional neural networks, over a million weights and associated parameters would have to be represented by a string of over a million numbers. This N-dimensional space of possible genomes would become entirely unfeasible to explore.\n",
    "\n",
    "With this problem in mind, there have been numerous attempts to develop indirect encodings of artificial neural networks which exploit the regularity in a particular model to compress it's representation to some set of parameters M << N. One of the most prominent proposals for dealing with this problem is the HyperNEAT algorithm (Hypercube-based NeuroEvolution of Augmenting Topologies).\n",
    "\n",
    "Originally, HyperNEAT's predecessor, NEAT, was in fact a direct encoding representation of a neural network solution to a controller problem. For example, a robot has a set of sensors that act as inputs for a neural network that generate outputs to it effectors in some control task. If we imagine some solution without and hidden nodes that is fully connected between S sensor input nodes and E effector output nodes, the network will have S x E weights that must be described by S x E genes. A solution is generated from the directly encoded genome and evaluated on the controller task. After the population is transformed from their genomes and evaluated, effective solutions are bred an mutated.\n",
    "\n",
    "NEAT is different from other neuroevolution techniques in many ways. First, it is common for an initial population of solutions to be randomly generated with potentially very different initial architectures. NEAT, however, begins an experiment with a population of identicallly organized networks without hidden nodes, such that individual solutions only differ in their connection weights. Second, since the architectures of these networks are identical, mutation has to be the driving force for diversity in successive generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
